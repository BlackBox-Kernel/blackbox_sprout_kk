--- virt/kvm/kvm_main.c
+++ virt/kvm/kvm_main.c
@@ -52,7 +52,6 @@
 
 #include <asm/processor.h>
 #include <asm/io.h>
-#include <asm/ioctl.h>
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
 
@@ -694,7 +693,8 @@
 	int r;
 	gfn_t base_gfn;
 	unsigned long npages;
-	struct kvm_memory_slot *memslot, *slot;
+	unsigned long i;
+	struct kvm_memory_slot *memslot;
 	struct kvm_memory_slot old, new;
 	struct kvm_memslots *slots, *old_memslots;
 
@@ -741,11 +741,13 @@
 
 	/* Check for overlaps */
 	r = -EEXIST;
-	kvm_for_each_memslot(slot, kvm->memslots) {
-		if (slot->id >= KVM_MEMORY_SLOTS || slot == memslot)
+	for (i = 0; i < KVM_MEMORY_SLOTS; ++i) {
+		struct kvm_memory_slot *s = &kvm->memslots->memslots[i];
+
+		if (s == memslot || !s->npages)
 			continue;
-		if (!((base_gfn + npages <= slot->base_gfn) ||
-		      (base_gfn >= slot->base_gfn + slot->npages)))
+		if (!((base_gfn + npages <= s->base_gfn) ||
+		      (base_gfn >= s->base_gfn + s->npages)))
 			goto out_free;
 	}
 
@@ -775,7 +777,7 @@
 		/* destroy any largepage mappings for dirty tracking */
 	}
 
-	if (!npages || base_gfn != old.base_gfn) {
+	if (!npages) {
 		struct kvm_memory_slot *slot;
 
 		r = -ENOMEM;
@@ -791,10 +793,8 @@
 		old_memslots = kvm->memslots;
 		rcu_assign_pointer(kvm->memslots, slots);
 		synchronize_srcu_expedited(&kvm->srcu);
-		/* slot was deleted or moved, clear iommu mapping */
-		kvm_iommu_unmap_pages(kvm, &old);
-		/* From this point no new shadow pages pointing to a deleted,
-		 * or moved, memslot will be created.
+		/* From this point no new shadow pages pointing to a deleted
+		 * memslot will be created.
 		 *
 		 * validation of sp->gfn happens in:
 		 * 	- gfn_to_hva (kvm_read_guest, gfn_to_pfn)
@@ -808,19 +808,20 @@
 	if (r)
 		goto out_free;
 
+	/* map/unmap the pages in iommu page table */
+	if (npages) {
+		r = kvm_iommu_map_pages(kvm, &new);
+		if (r)
+			goto out_free;
+	} else
+		kvm_iommu_unmap_pages(kvm, &old);
+
 	r = -ENOMEM;
 	slots = kmemdup(kvm->memslots, sizeof(struct kvm_memslots),
 			GFP_KERNEL);
 	if (!slots)
 		goto out_free;
 
-	/* map new memory slot into the iommu */
-	if (npages) {
-		r = kvm_iommu_map_pages(kvm, &new);
-		if (r)
-			goto out_slots;
-	}
-
 	/* actual memory is freed via old in kvm_free_physmem_slot below */
 	if (!npages) {
 		new.rmap = NULL;
@@ -847,8 +848,6 @@
 
 	return 0;
 
-out_slots:
-	kfree(slots);
 out_free:
 	kvm_free_physmem_slot(&new, &old);
 out:
@@ -1386,38 +1385,21 @@
 }
 
 int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
-			      gpa_t gpa, unsigned long len)
+			      gpa_t gpa)
 {
 	struct kvm_memslots *slots = kvm_memslots(kvm);
 	int offset = offset_in_page(gpa);
-	gfn_t start_gfn = gpa >> PAGE_SHIFT;
-	gfn_t end_gfn = (gpa + len - 1) >> PAGE_SHIFT;
-	gfn_t nr_pages_needed = end_gfn - start_gfn + 1;
-	gfn_t nr_pages_avail;
+	gfn_t gfn = gpa >> PAGE_SHIFT;
 
 	ghc->gpa = gpa;
 	ghc->generation = slots->generation;
-	ghc->len = len;
-	ghc->memslot = gfn_to_memslot(kvm, start_gfn);
-	ghc->hva = gfn_to_hva_many(ghc->memslot, start_gfn, &nr_pages_avail);
-	if (!kvm_is_error_hva(ghc->hva) && nr_pages_avail >= nr_pages_needed) {
+	ghc->memslot = gfn_to_memslot(kvm, gfn);
+	ghc->hva = gfn_to_hva_many(ghc->memslot, gfn, NULL);
+	if (!kvm_is_error_hva(ghc->hva))
 		ghc->hva += offset;
-	} else {
-		/*
-		 * If the requested region crosses two memslots, we still
-		 * verify that the entire region is valid here.
-		 */
-		while (start_gfn <= end_gfn) {
-			ghc->memslot = gfn_to_memslot(kvm, start_gfn);
-			ghc->hva = gfn_to_hva_many(ghc->memslot, start_gfn,
-						   &nr_pages_avail);
-			if (kvm_is_error_hva(ghc->hva))
-				return -EFAULT;
-			start_gfn += nr_pages_avail;
-		}
-		/* Use the slow path for cross page reads and writes. */
-		ghc->memslot = NULL;
-	}
+	else
+		return -EFAULT;
+
 	return 0;
 }
 EXPORT_SYMBOL_GPL(kvm_gfn_to_hva_cache_init);
@@ -1428,13 +1410,8 @@
 	struct kvm_memslots *slots = kvm_memslots(kvm);
 	int r;
 
-	BUG_ON(len > ghc->len);
-
 	if (slots->generation != ghc->generation)
-		kvm_gfn_to_hva_cache_init(kvm, ghc, ghc->gpa, ghc->len);
-
-	if (unlikely(!ghc->memslot))
-		return kvm_write_guest(kvm, ghc->gpa, data, len);
+		kvm_gfn_to_hva_cache_init(kvm, ghc, ghc->gpa);
 
 	if (kvm_is_error_hva(ghc->hva))
 		return -EFAULT;
@@ -1454,13 +1431,8 @@
 	struct kvm_memslots *slots = kvm_memslots(kvm);
 	int r;
 
-	BUG_ON(len > ghc->len);
-
 	if (slots->generation != ghc->generation)
-		kvm_gfn_to_hva_cache_init(kvm, ghc, ghc->gpa, ghc->len);
-
-	if (unlikely(!ghc->memslot))
-		return kvm_read_guest(kvm, ghc->gpa, data, len);
+		kvm_gfn_to_hva_cache_init(kvm, ghc, ghc->gpa);
 
 	if (kvm_is_error_hva(ghc->hva))
 		return -EFAULT;
@@ -1669,9 +1641,6 @@
 	int r;
 	struct kvm_vcpu *vcpu, *v;
 
-	if (id >= KVM_MAX_VCPUS)
-		return -EINVAL;
-
 	vcpu = kvm_arch_vcpu_create(kvm, id);
 	if (IS_ERR(vcpu))
 		return PTR_ERR(vcpu);
@@ -1745,9 +1714,6 @@
 	if (vcpu->kvm->mm != current->mm)
 		return -EIO;
 
-	if (unlikely(_IOC_TYPE(ioctl) != KVMIO))
-		return -EINVAL;
-
 #if defined(CONFIG_S390) || defined(CONFIG_PPC)
 	/*
 	 * Special cases: vcpu ioctls that are asynchronous to vcpu execution,
